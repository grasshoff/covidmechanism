{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "capital-shield",
   "metadata": {},
   "source": [
    "# Covid causal mechanism\n",
    "Gerd Gra√ühoff, Humboldt University of Berlin, Max Planck Institute for History of Science, BIFOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-producer",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Analysis of abstracts of Covid publications (more than 210 000) provides a set of terms used for expressing causal relationships. Often they are identified as \"mechanism\". We group them:\n",
    "\n",
    "## Cause\n",
    "\n",
    "- cause\n",
    "- factor\n",
    "\n",
    "## Effects\n",
    "\n",
    "- disease\n",
    "- events\n",
    "\n",
    "## causal relevance\n",
    "\n",
    "### positive\n",
    "\n",
    "- increase\n",
    "- stimulate\n",
    "\n",
    "### negative\n",
    "\n",
    "- inhibits\n",
    "- prevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "basic-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "played-beverage",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qualified-blank",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-sullivan",
   "metadata": {},
   "source": [
    "# Load dimension covid publication dataframe \n",
    "Note that the data directory is parallel to the notebook directory to save github storage space.\n",
    "The data files are hosted in figshare and its file name need to be renamed appropriately\n",
    "\n",
    "https://dimensions.figshare.com/articles/dataset/Dimensions_COVID-19_publications_datasets_and_clinical_trials/11961063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comfortable-baker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 216722 entries, 0 to 216721\n",
      "Data columns (total 31 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   Date added                             216722 non-null  object \n",
      " 1   Publication ID                         216722 non-null  object \n",
      " 2   DOI                                    212447 non-null  object \n",
      " 3   PMID                                   98507 non-null   float64\n",
      " 4   PMCID                                  75794 non-null   object \n",
      " 5   Title                                  216722 non-null  object \n",
      " 6   Abstract                               142644 non-null  object \n",
      " 7   Source title                           197976 non-null  object \n",
      " 8   Source UID                             197976 non-null  object \n",
      " 9   Publisher                              202095 non-null  object \n",
      " 10  MeSH terms                             40609 non-null   object \n",
      " 11  Publication Date                       216722 non-null  object \n",
      " 12  PubYear                                216722 non-null  int64  \n",
      " 13  Volume                                 139324 non-null  object \n",
      " 14  Issue                                  118076 non-null  object \n",
      " 15  Pagination                             161681 non-null  object \n",
      " 16  Open Access                            216722 non-null  object \n",
      " 17  Publication Type                       216722 non-null  object \n",
      " 18  Authors                                204321 non-null  object \n",
      " 19  Corresponding Authors                  64194 non-null   object \n",
      " 20  Authors Affiliations                   204321 non-null  object \n",
      " 21  Research Organizations - standardized  126534 non-null  object \n",
      " 22  GRID IDs                               126534 non-null  object \n",
      " 23  City of Research organization          126534 non-null  object \n",
      " 24  Country of Research organization       126534 non-null  object \n",
      " 25  Funder                                 22440 non-null   object \n",
      " 26  UIDs of supporting grants              11386 non-null   object \n",
      " 27  Times cited                            216722 non-null  int64  \n",
      " 28  Altmetric                              100513 non-null  float64\n",
      " 29  Source Linkout                         159645 non-null  object \n",
      " 30  Dimensions URL                         216722 non-null  object \n",
      "dtypes: float64(2), int64(2), object(27)\n",
      "memory usage: 51.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# takes some time to read raw data, then creates a parquet data format for faster loading\n",
    "# is uncommented once parquet data are available\n",
    "df=pd.read_excel(\"../coviddata/dimensions-covid-2021-Feb-19.xlsx\")\n",
    "df.to_parquet('../coviddata/covid.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "junior-allen",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Compression 'snappy' not available.  Options: ['BROTLI', 'GZIP', 'UNCOMPRESSED']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dd688f7fbb8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../coviddata/covid.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../coviddata/file.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         return to_parquet(\n\u001b[0m\u001b[1;32m   2456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFilePathOrBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     impl.write(\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             self.api.write(\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(filename, data, row_group_offsets, compression, file_scheme, open_with, mkdirs, has_nulls, write_index, partition_on, fixed_text, append, object_encoding, times)\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m         write_simple(filename, data, fmd, row_group_offsets,\n\u001b[0m\u001b[1;32m    939\u001b[0m                      compression, open_with, has_nulls, append)\n\u001b[1;32m    940\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfile_scheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drill'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite_simple\u001b[0;34m(fn, data, fmd, row_group_offsets, compression, open_with, has_nulls, append)\u001b[0m\n\u001b[1;32m    800\u001b[0m             end = (row_group_offsets[i+1] if i < (len(row_group_offsets) - 1)\n\u001b[1;32m    801\u001b[0m                    else None)\n\u001b[0;32m--> 802\u001b[0;31m             rg = make_row_group(f, data[start:end], fmd.schema,\n\u001b[0m\u001b[1;32m    803\u001b[0m                                 compression=compression)\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mmake_row_group\u001b[0;34m(f, data, schema, compression)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             chunk = write_column(f, data[column.name], column,\n\u001b[0m\u001b[1;32m    672\u001b[0m                                  compression=comp)\n\u001b[1;32m    673\u001b[0m             \u001b[0mrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/fastparquet/writer.py\u001b[0m in \u001b[0;36mwrite_column\u001b[0;34m(f, data, selement, compression)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mbdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompress_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/spacy3/lib/python3.8/site-packages/fastparquet/compression.py\u001b[0m in \u001b[0;36mcompress_data\u001b[0;34m(data, compression)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompressions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         raise RuntimeError(\"Compression '%s' not available.  Options: %s\" %\n\u001b[0m\u001b[1;32m    114\u001b[0m                 (algorithm, sorted(compressions)))\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Compression 'snappy' not available.  Options: ['BROTLI', 'GZIP', 'UNCOMPRESSED']"
     ]
    }
   ],
   "source": [
    "df.to_parquet('../coviddata/covid.parquet')\n",
    "pd.read_parquet('../coviddata/file.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter subset of those publications with abstract \"mechanism\"\n",
    "dff=df[df[\"Abstract\"].str.contains(\"mechanism\",na=False)]\n",
    "print(len(dff))\n",
    "dff.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual selection of example to analyse\n",
    "example=dff.iloc[4953] # 233\n",
    "example[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-center",
   "metadata": {},
   "source": [
    "# NLP of sentences in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2DF():\n",
    "    sents=[]\n",
    "    for par,row in dff.iterrows():\n",
    "        sentences=nlp(row[\"Abstract\"]).sents\n",
    "        chID=row[\"PMID\"]\n",
    "        for sentID,sent in enumerate(sentences):\n",
    "            sents.append({\"chID\":chID,\"sentID\":sentID,\"sent\":sent})\n",
    "    return(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=data2DF()\n",
    "print(f\"number of sentences: {len(sents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-basis",
   "metadata": {},
   "source": [
    "## Filter sentences\n",
    "Filtering sentence items increases the efficancy of subsequent processing for information extraction and semantic modelling. It should be fast enough to reduce efficiently\n",
    "\n",
    "Filter categories operate on the token level of spacy processed sentence docs. It can therefore filter with enriched attributes from the spacy nlp:\n",
    "\n",
    "- matches for the following keys using their values:\n",
    "    - \"text\"\n",
    "    - \"lemma\"\n",
    "    - \"dep\"\n",
    "    - \"pos\"\n",
    "    - \"compound\"\n",
    "    - \"pattern\" \n",
    "    \n",
    "- match pattern is provided by a JSON object: a list of dicts. \n",
    "    - each item of the list is matched on each token.\n",
    "        e.g. [{\"lemma\":\"law\"}] matches if a token has a lemma==\"law\"\n",
    "    \n",
    "Logic of matches: at least one match of a dict on a token of a sentence matches the entire sentence, hence each dict of the list is an or-condition. Each dict element then forms an and-condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmat(t,dfi):\n",
    "    switch={\"lemma\":t.lemma_,\n",
    "           \"pos\":t.pos_,\n",
    "           \"dep\":t.dep_,\n",
    "            \"text\":t.text}\n",
    "    logs=False\n",
    "    for pat in dfi:\n",
    "        likeys=pat.keys()\n",
    "        for k in likeys:\n",
    "            wert=switch[k]\n",
    "            pt=pat[k]\n",
    "            if pt==wert:\n",
    "                logs=True\n",
    "    return(logs)\n",
    "        \n",
    "        \n",
    "def filtsent(row,dfi):\n",
    "    sent=row[\"sent\"]\n",
    "    lfi=False\n",
    "    for t in sent:\n",
    "        if lmat(t,dfi):\n",
    "            lfi=True\n",
    "            break\n",
    "    return(lfi)\n",
    "\n",
    "def filterdf(df,fdict):\n",
    "    ''' \n",
    "        df dataframe with sentences after nlp processing,\n",
    "        fdict: dictionary with filter categories and match terms\n",
    "    '''\n",
    "    return(df[df.apply(lambda x:filtsent(x,fdict),axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1=[{\"lemma\":\"mechanism\"}]\n",
    "# used for training puposes for selecting few cases\n",
    "dff=df.iloc[:]\n",
    "filterdf(dff,pat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "    print(subtree_span.root.text,\"::\",word.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "   # print(\"\".join(w.text_with_ws for w in word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    if word.dep_ in (\"ROOT\"):\n",
    "        subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "        print(subtree_span.root.text,\"::\",word.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "        chds=[t.text for t in word.children]\n",
    "        print(\"children:\",chds)\n",
    "        for t in doc:\n",
    "            if t.text in chds:\n",
    "                subtree_span = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "                print(subtree_span.root.text,\"::\",t.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(s.to_dict())\n",
    "df[[\"id\",\"text\",\"upos\",\"head\",\"deprel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(deplacy.dot(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "semtree=MultiDiGraph()\n",
    "for i,e in df.iterrows():\n",
    "    semtree.add_edge(e[\"id\"],e[\"head\"],label=e[\"deprel\"],arrowsize=1, arrowstyle='fancy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
