{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educational-rebate",
   "metadata": {},
   "source": [
    "# Covid causal mechanism\n",
    "Gerd Gra√ühoff, Humboldt University of Berlin, Max Planck Institute for History of Science, BIFOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-listing",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Analysis of abstracts of Covid publications (more than 210 000) provides a set of terms used for expressing causal relationships. Often they are identified as \"mechanism\". We group them:\n",
    "\n",
    "## Cause\n",
    "\n",
    "- cause\n",
    "- factor\n",
    "\n",
    "## Effects\n",
    "\n",
    "- disease\n",
    "- events\n",
    "\n",
    "## causal relevance\n",
    "\n",
    "### positive\n",
    "\n",
    "- increase\n",
    "- stimulate\n",
    "\n",
    "### negative\n",
    "\n",
    "- inhibits\n",
    "- prevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "italic-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caring-traveler",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.1\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-sequence",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-geometry",
   "metadata": {},
   "source": [
    "# Load dimension covid publication dataframe \n",
    "Note that the data directory is parallel to the notebook directory to save github storage space.\n",
    "The data files are hosted in figshare and its file name need to be renamed appropriately\n",
    "\n",
    "https://dimensions.figshare.com/articles/dataset/Dimensions_COVID-19_publications_datasets_and_clinical_trials/11961063\n",
    "\n",
    "loads data in compressed format\n",
    "\n",
    "install fastparquet\n",
    "\n",
    "conda install -c conda-forge python-snappy fastparquet snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unusual-travel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 216722 entries, 0 to 216721\n",
      "Data columns (total 31 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   Date added                             216722 non-null  object \n",
      " 1   Publication ID                         216722 non-null  object \n",
      " 2   DOI                                    212447 non-null  object \n",
      " 3   PMID                                   98507 non-null   float64\n",
      " 4   PMCID                                  75794 non-null   object \n",
      " 5   Title                                  216722 non-null  object \n",
      " 6   Abstract                               142644 non-null  object \n",
      " 7   Source title                           197976 non-null  object \n",
      " 8   Source UID                             197976 non-null  object \n",
      " 9   Publisher                              202095 non-null  object \n",
      " 10  MeSH terms                             40609 non-null   object \n",
      " 11  Publication Date                       216722 non-null  object \n",
      " 12  PubYear                                216722 non-null  int64  \n",
      " 13  Volume                                 139324 non-null  object \n",
      " 14  Issue                                  118076 non-null  object \n",
      " 15  Pagination                             161681 non-null  object \n",
      " 16  Open Access                            216722 non-null  object \n",
      " 17  Publication Type                       216722 non-null  object \n",
      " 18  Authors                                204321 non-null  object \n",
      " 19  Corresponding Authors                  64194 non-null   object \n",
      " 20  Authors Affiliations                   204321 non-null  object \n",
      " 21  Research Organizations - standardized  126534 non-null  object \n",
      " 22  GRID IDs                               126534 non-null  object \n",
      " 23  City of Research organization          126534 non-null  object \n",
      " 24  Country of Research organization       126534 non-null  object \n",
      " 25  Funder                                 22440 non-null   object \n",
      " 26  UIDs of supporting grants              11386 non-null   object \n",
      " 27  Times cited                            216722 non-null  int64  \n",
      " 28  Altmetric                              100513 non-null  float64\n",
      " 29  Source Linkout                         159645 non-null  object \n",
      " 30  Dimensions URL                         216722 non-null  object \n",
      "dtypes: float64(2), int64(2), object(27)\n",
      "memory usage: 51.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# takes some time to read raw data, then creates a parquet data format for faster loading\n",
    "# is uncommented once parquet data are available\n",
    "df=pd.read_excel(\"../coviddata/dimensions-covid-2021-Feb-19.xlsx\")\n",
    "df.to_parquet('../coviddata/covid.parquet')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solid-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet('../coviddata/covid.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter subset of those publications with abstract \"mechanism\"\n",
    "dff=df[df[\"Abstract\"].str.contains(\"mechanism\",na=False)]\n",
    "print(len(dff))\n",
    "dff.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual selection of example to analyse\n",
    "example=dff.iloc[4953] # 233\n",
    "example[\"Abstract\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-illustration",
   "metadata": {},
   "source": [
    "# NLP of sentences in abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2DF():\n",
    "    sents=[]\n",
    "    for par,row in dff.iterrows():\n",
    "        sentences=nlp(row[\"Abstract\"]).sents\n",
    "        chID=row[\"PMID\"]\n",
    "        for sentID,sent in enumerate(sentences):\n",
    "            sents.append({\"chID\":chID,\"sentID\":sentID,\"sent\":sent})\n",
    "    return(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=data2DF()\n",
    "print(f\"number of sentences: {len(sents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-revelation",
   "metadata": {},
   "source": [
    "## Filter sentences\n",
    "Filtering sentence items increases the efficancy of subsequent processing for information extraction and semantic modelling. It should be fast enough to reduce efficiently\n",
    "\n",
    "Filter categories operate on the token level of spacy processed sentence docs. It can therefore filter with enriched attributes from the spacy nlp:\n",
    "\n",
    "- matches for the following keys using their values:\n",
    "    - \"text\"\n",
    "    - \"lemma\"\n",
    "    - \"dep\"\n",
    "    - \"pos\"\n",
    "    - \"compound\"\n",
    "    - \"pattern\" \n",
    "    \n",
    "- match pattern is provided by a JSON object: a list of dicts. \n",
    "    - each item of the list is matched on each token.\n",
    "        e.g. [{\"lemma\":\"law\"}] matches if a token has a lemma==\"law\"\n",
    "    \n",
    "Logic of matches: at least one match of a dict on a token of a sentence matches the entire sentence, hence each dict of the list is an or-condition. Each dict element then forms an and-condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmat(t,dfi):\n",
    "    switch={\"lemma\":t.lemma_,\n",
    "           \"pos\":t.pos_,\n",
    "           \"dep\":t.dep_,\n",
    "            \"text\":t.text}\n",
    "    logs=False\n",
    "    for pat in dfi:\n",
    "        likeys=pat.keys()\n",
    "        for k in likeys:\n",
    "            wert=switch[k]\n",
    "            pt=pat[k]\n",
    "            if pt==wert:\n",
    "                logs=True\n",
    "    return(logs)\n",
    "        \n",
    "        \n",
    "def filtsent(row,dfi):\n",
    "    sent=row[\"sent\"]\n",
    "    lfi=False\n",
    "    for t in sent:\n",
    "        if lmat(t,dfi):\n",
    "            lfi=True\n",
    "            break\n",
    "    return(lfi)\n",
    "\n",
    "def filterdf(df,fdict):\n",
    "    ''' \n",
    "        df dataframe with sentences after nlp processing,\n",
    "        fdict: dictionary with filter categories and match terms\n",
    "    '''\n",
    "    return(df[df.apply(lambda x:filtsent(x,fdict),axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1=[{\"lemma\":\"mechanism\"}]\n",
    "# used for training puposes for selecting few cases\n",
    "dff=df.iloc[:]\n",
    "filterdf(dff,pat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "    print(subtree_span.root.text,\"::\",word.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "   # print(\"\".join(w.text_with_ws for w in word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in doc:\n",
    "    if word.dep_ in (\"ROOT\"):\n",
    "        subtree_span = doc[word.left_edge.i : word.right_edge.i + 1]\n",
    "        print(subtree_span.root.text,\"::\",word.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "        chds=[t.text for t in word.children]\n",
    "        print(\"children:\",chds)\n",
    "        for t in doc:\n",
    "            if t.text in chds:\n",
    "                subtree_span = doc[t.left_edge.i : t.right_edge.i + 1]\n",
    "                print(subtree_span.root.text,\"::\",t.dep_,\"::\",\"--->\",subtree_span.text,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(s.to_dict())\n",
    "df[[\"id\",\"text\",\"upos\",\"head\",\"deprel\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphviz.Source(deplacy.dot(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "semtree=MultiDiGraph()\n",
    "for i,e in df.iterrows():\n",
    "    semtree.add_edge(e[\"id\"],e[\"head\"],label=e[\"deprel\"],arrowsize=1, arrowstyle='fancy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
